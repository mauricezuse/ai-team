# AI-Team Testing Guide

This guide covers the comprehensive testing suite for the AI-Team project, including unit tests, integration tests, and Playwright E2E tests.

## Test Structure

```
tests/
├── test_api_endpoints.py          # Unit tests for API endpoints
├── test_workflow_integration.py   # Integration tests for workflows
├── playwright/
│   ├── test_workflow_display.spec.ts      # E2E tests for workflow display
│   ├── test_agent_conversations.spec.ts   # E2E tests for agent conversations
│   └── playwright.config.ts               # Playwright configuration
├── run_tests.py                   # Test runner script
└── requirements-test.txt          # Testing dependencies
```

## Test Categories

### 1. Unit Tests (`test_api_endpoints.py`)

**Purpose**: Test individual API endpoints in isolation

**Coverage**:
- ✅ GET `/workflows` - List all workflows
- ✅ GET `/workflows/{workflow_id}` - Get workflow details
- ✅ POST `/workflows/{workflow_id}/execute` - Execute workflow
- ✅ GET `/agents` - List all agents
- ✅ GET `/agents/{agent_id}` - Get agent details
- ✅ GET `/health` - Health check
- ✅ GET `/` - Endpoint listing

**Key Test Cases**:
- Empty workflow list handling
- Workflow data parsing and structure
- Agent data completeness
- Error handling (404, 500 responses)
- CORS configuration
- Workflow execution with different story IDs

### 2. Integration Tests (`test_workflow_integration.py`)

**Purpose**: Test workflow execution and agent collaboration end-to-end

**Coverage**:
- ✅ Workflow lifecycle (creation → execution → completion)
- ✅ Agent collaboration data structure
- ✅ Escalation and collaboration handling
- ✅ Workflow error handling and recovery
- ✅ Agent capability data consistency
- ✅ Workflow data persistence

**Key Test Cases**:
- Complete workflow execution with realistic data
- Agent collaboration and escalation flows
- Workflow checkpoint creation and loading
- Error handling with malformed data
- Agent role and capability consistency

### 3. Playwright E2E Tests

#### Workflow Display Tests (`test_workflow_display.spec.ts`)

**Purpose**: Test frontend workflow display functionality

**Coverage**:
- ✅ Workflow list display
- ✅ Workflow detail navigation
- ✅ Agent conversation display
- ✅ Code file display
- ✅ Workflow execution from UI
- ✅ Error handling in UI
- ✅ Status indicators and filtering

#### Agent Conversations Tests (`test_agent_conversations.spec.ts`)

**Purpose**: Test agent conversation display and interaction

**Coverage**:
- ✅ Chronological conversation display
- ✅ Agent names and roles
- ✅ Conversation details and outputs
- ✅ Code files generated by agents
- ✅ Escalations and collaborations
- ✅ Conversation filtering and search
- ✅ Conversation flow visualization

## Running Tests

### Prerequisites

1. **Install Python dependencies**:
   ```bash
   pip install -r requirements-test.txt
   ```

2. **Install Playwright browsers** (for E2E tests):
   ```bash
   cd frontend
   npx playwright install --with-deps
   ```

### Test Execution

#### Run All Tests
```bash
python run_tests.py
```

#### Run Specific Test Categories

**Unit Tests Only**:
```bash
python -m pytest tests/test_api_endpoints.py -v
```

**Integration Tests Only**:
```bash
python -m pytest tests/test_workflow_integration.py -v
```

**Playwright Tests Only**:
```bash
cd frontend
npx playwright test ../tests/playwright --reporter=html
```

**All Python Tests**:
```bash
python -m pytest tests/ -v --tb=short
```

### Test Configuration

#### Playwright Configuration
- **Base URL**: `http://localhost:4200` (Angular dev server)
- **Browsers**: Chrome, Firefox, Safari, Mobile Chrome, Mobile Safari
- **Parallel Execution**: Enabled
- **Retries**: 2 on CI, 0 locally
- **Trace Collection**: On first retry

#### Pytest Configuration
- **Verbose Output**: `-v` flag
- **Short Traceback**: `--tb=short`
- **Parallel Execution**: Available with `pytest-xdist`

## Test Data and Mocking

### API Mocking Strategy

**Workflow Data Mocking**:
```typescript
const mockWorkflows = [
  {
    id: 'enhanced_story_NEGISHI-178',
    name: 'Enhanced Story Negishi-178',
    status: 'completed',
    agents: ['pm', 'architect', 'developer'],
    created_at: '2024-01-01T00:00:00',
    last_step: 'developer_implementation'
  }
];
```

**Agent Conversation Mocking**:
```typescript
const mockConversations = [
  {
    step: 'story_retrieved',
    timestamp: '2024-01-01T00:00:00',
    agent: 'pm',
    status: 'completed',
    details: 'Story retrieved successfully',
    output: 'Story details...',
    code_files: [],
    escalations: [],
    collaborations: []
  }
];
```

### Test Data Patterns

1. **Realistic Workflow Data**: Complete workflow logs with multiple agents
2. **Agent Collaboration**: Escalations and collaborations between agents
3. **Code Generation**: Multiple code files per agent
4. **Error Scenarios**: Network errors, malformed data, missing files
5. **Edge Cases**: Empty data, large datasets, concurrent operations

## Test Coverage

### API Endpoints Coverage
- ✅ **100%** of new endpoints covered
- ✅ **Error handling** for all endpoints
- ✅ **Data validation** for request/response
- ✅ **CORS configuration** verified

### Workflow Integration Coverage
- ✅ **Complete workflow lifecycle** tested
- ✅ **Agent collaboration** flows verified
- ✅ **Data persistence** and retrieval tested
- ✅ **Error recovery** mechanisms verified

### Frontend E2E Coverage
- ✅ **Workflow display** functionality
- ✅ **Agent conversation** display
- ✅ **User interactions** (filtering, searching, execution)
- ✅ **Error handling** in UI
- ✅ **Responsive design** (mobile/desktop)

## Continuous Integration

### GitHub Actions Integration
```yaml
name: Test Suite
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Install dependencies
        run: |
          pip install -r requirements-test.txt
      - name: Run unit tests
        run: python -m pytest tests/test_api_endpoints.py -v
      - name: Run integration tests
        run: python -m pytest tests/test_workflow_integration.py -v
      - name: Run Playwright tests
        run: |
          cd frontend
          npm install
          npx playwright install --with-deps
          npx playwright test ../tests/playwright
```

## Test Maintenance

### Adding New Tests

1. **Unit Tests**: Add to `test_api_endpoints.py` for new endpoints
2. **Integration Tests**: Add to `test_workflow_integration.py` for workflow features
3. **E2E Tests**: Add to appropriate Playwright spec files

### Test Data Updates

1. **Mock Data**: Update mock data in test files when API changes
2. **Test Scenarios**: Add new test cases for new features
3. **Error Cases**: Add tests for new error scenarios

### Performance Testing

**Load Testing** (Future Enhancement):
```python
# Example load test for API endpoints
import asyncio
import aiohttp

async def load_test_workflows():
    async with aiohttp.ClientSession() as session:
        tasks = []
        for i in range(100):
            task = session.get('http://localhost:8000/workflows')
            tasks.append(task)
        responses = await asyncio.gather(*tasks)
        return responses
```

## Troubleshooting

### Common Issues

1. **Playwright Browser Issues**:
   ```bash
   cd frontend
   npx playwright install --with-deps
   ```

2. **Python Import Errors**:
   ```bash
   export PYTHONPATH="${PYTHONPATH}:$(pwd)"
   ```

3. **FastAPI Test Client Issues**:
   ```bash
   pip install httpx
   ```

4. **Mock Data Issues**:
   - Verify mock data structure matches API responses
   - Check route patterns in Playwright tests
   - Ensure mock data includes all required fields

### Debug Mode

**Verbose Test Output**:
```bash
python -m pytest tests/ -v -s --tb=long
```

**Playwright Debug Mode**:
```bash
cd frontend
npx playwright test --debug
```

**API Debug Mode**:
```bash
python -m pytest tests/test_api_endpoints.py -v -s --capture=no
```

## Best Practices

### Test Writing Guidelines

1. **Descriptive Test Names**: Use clear, descriptive test function names
2. **Arrange-Act-Assert**: Structure tests with clear setup, execution, and verification
3. **Mock External Dependencies**: Use mocks for external services and APIs
4. **Test Data Isolation**: Each test should be independent
5. **Error Scenarios**: Test both success and failure cases

### Code Coverage

**Target Coverage**:
- **Unit Tests**: 90%+ coverage for API endpoints
- **Integration Tests**: 80%+ coverage for workflow execution
- **E2E Tests**: 70%+ coverage for critical user flows

**Coverage Reporting**:
```bash
python -m pytest tests/ --cov=crewai_app --cov-report=html
```

## Future Enhancements

### Planned Test Improvements

1. **Performance Tests**: Load testing for API endpoints
2. **Security Tests**: Authentication and authorization testing
3. **Accessibility Tests**: WCAG compliance testing
4. **Visual Regression Tests**: UI screenshot comparison
5. **API Contract Tests**: OpenAPI specification validation

### Test Automation

1. **Pre-commit Hooks**: Run tests before commits
2. **Scheduled Tests**: Daily/weekly test runs
3. **Test Reports**: Automated test reporting
4. **Test Metrics**: Track test performance and coverage trends
